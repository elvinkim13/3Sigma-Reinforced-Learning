{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from Gridworld import GridWorld\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = GridWorld(negative_reward=-10, bomb_positions=[(0, 2), (1, 3)], gold_positions=[(0, 3)])\n",
    "env = GridWorld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0    1    2    3    4\n",
      "-------------------------\n",
      "0 |           p    g         \n",
      "1 |                x         \n",
      "2 |                          \n",
      "3 |                          \n",
      "4 |                          \n"
     ]
    }
   ],
   "source": [
    "env.print_world()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, discount_factor=0.9, epsilon=1e-5):\n",
    "    policy = np.zeros([env.num_rows, env.num_cols])\n",
    "    V = np.zeros([env.num_rows, env.num_cols])\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(env.num_rows):\n",
    "            for j in range(env.num_cols):\n",
    "                state = (i, j)\n",
    "                if env.is_terminal(state):\n",
    "                    continue\n",
    "                action_values = np.zeros(4)\n",
    "                for k, action in enumerate(env.actions):\n",
    "                    new_state, r = env.make_step(state, action)\n",
    "                    action_values[k] = r + discount_factor * V[new_state]\n",
    "\n",
    "                best_action_value = np.max(action_values)\n",
    "                best_action = np.argmax(action_values)\n",
    "\n",
    "                delta = max(delta, np.abs(best_action_value - V[state]))\n",
    "\n",
    "                V[state] = best_action_value\n",
    "                policy[state] = best_action\n",
    "\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "    \n",
    "    policy = [[env.actions[int(i)] for i in j] for j in policy]\n",
    "    for term in env.terminal_states:\n",
    "        policy[term[0]][term[1]] = 'x'\n",
    "    \n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, policy = value_iteration(env, discount_factor=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 8.1    ,  9.     , 10.     ,  0.     , 10.     ],\n",
       "        [ 7.29   ,  8.1    ,  9.     ,  0.     ,  9.     ],\n",
       "        [ 6.561  ,  7.29   ,  8.1    ,  7.29   ,  8.1    ],\n",
       "        [ 5.9049 ,  6.561  ,  7.29   ,  6.561  ,  7.29   ],\n",
       "        [ 5.31441,  5.9049 ,  6.561  ,  5.9049 ,  6.561  ]]),\n",
       " [['e', 'e', 'e', 'x', 'w'],\n",
       "  ['n', 'n', 'n', 'x', 'n'],\n",
       "  ['n', 'n', 'n', 'e', 'n'],\n",
       "  ['n', 'n', 'n', 'n', 'n'],\n",
       "  ['n', 'n', 'n', 'n', 'n']])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game start!\n",
      "    0    1    2    3    4\n",
      "-------------------------\n",
      "0 |                g         \n",
      "1 |                x         \n",
      "2 |                          \n",
      "3 |                          \n",
      "4 | p                        \n",
      "\n",
      "move 1: n\n",
      "    0    1    2    3    4\n",
      "-------------------------\n",
      "0 |                g         \n",
      "1 |                x         \n",
      "2 |                          \n",
      "3 | p                        \n",
      "4 |                          \n",
      "\n",
      "move 2: n\n",
      "    0    1    2    3    4\n",
      "-------------------------\n",
      "0 |                g         \n",
      "1 |                x         \n",
      "2 | p                        \n",
      "3 |                          \n",
      "4 |                          \n",
      "\n",
      "move 3: n\n",
      "    0    1    2    3    4\n",
      "-------------------------\n",
      "0 |                g         \n",
      "1 | p              x         \n",
      "2 |                          \n",
      "3 |                          \n",
      "4 |                          \n",
      "\n",
      "move 4: n\n",
      "    0    1    2    3    4\n",
      "-------------------------\n",
      "0 | p              g         \n",
      "1 |                x         \n",
      "2 |                          \n",
      "3 |                          \n",
      "4 |                          \n",
      "\n",
      "move 5: e\n",
      "    0    1    2    3    4\n",
      "-------------------------\n",
      "0 |      p         g         \n",
      "1 |                x         \n",
      "2 |                          \n",
      "3 |                          \n",
      "4 |                          \n",
      "\n",
      "move 6: e\n",
      "    0    1    2    3    4\n",
      "-------------------------\n",
      "0 |           p    g         \n",
      "1 |                x         \n",
      "2 |                          \n",
      "3 |                          \n",
      "4 |                          \n",
      "\n",
      "move 7: e\n",
      "    0    1    2    3    4\n",
      "-------------------------\n",
      "0 |                p         \n",
      "1 |                x         \n",
      "2 |                          \n",
      "3 |                          \n",
      "4 |                          \n",
      "\n",
      "game over!\n"
     ]
    }
   ],
   "source": [
    "env.reset(start_pos=(4, 0))\n",
    "env.follow_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "What if we make the bombs okay to step on? When you step on the bomb, you get negative reward, but the game goes on. In that case, is it possible that the optimal policy guides the agent to the bomb location?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "What would happen when the gold position is not terminal, i.e. when you step on gold position, it is not game over. How will the value change? How will the policy change? Will the game ever end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9f1b1ba030845a5df2721083c94ad527972bf7403acc66881a1d33996209a40"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('synch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
